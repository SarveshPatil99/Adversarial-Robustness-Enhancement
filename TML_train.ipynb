{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhJV6SmW64zKCU+ZUTJJfA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshPatil99/Adversarial-Robustness-Enhancement/blob/main/TML_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1NGXGfoipcjVbIVXD6wIuxicaNlOyEvPG/view?usp=drive_link"
      ],
      "metadata": {
        "id": "Xu3MW73mlvLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gaussian_0.01.zip"
      ],
      "metadata": {
        "id": "UDVaukQolwBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "from tensorflow.keras.layers import Input, Conv2D, SeparableConv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, ReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "--q0Na8a9wLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_type = 'gaussian_0.01'"
      ],
      "metadata": {
        "id": "Gm0bJLQAl26D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = f'/content/noisy_data/{noise_type}'\n",
        "train_dir = pathlib.Path(f'{dataset_name}/train')\n",
        "val_dir = pathlib.Path(f'{dataset_name}/val')\n",
        "test_dir = pathlib.Path(f'{dataset_name}/test')\n",
        "train_len = len(list(train_dir.glob('*/*.png')))\n",
        "val_len = len(list(val_dir.glob('*/*.png')))\n",
        "classes = np.array(os.listdir(f'{dataset_name}/train'))\n",
        "class_dict = dict(zip(classes,range(len(classes))))\n",
        "print(train_len, val_len)\n",
        "print(class_dict)\n",
        "\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "batch_size = 16\n",
        "\n",
        "def create_label(image_path):\n",
        "  class_name = tf.strings.split(image_path,'/')[-2]\n",
        "  return tf.cast(classes == class_name,tf.float32)\n",
        "\n",
        "def load(image_path):\n",
        "\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_png(image) / 255\n",
        "  # image = tf.image.resize(image, [img_height, img_width])\n",
        "\n",
        "  label = create_label(image_path)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "train_ds = tf.data.Dataset.list_files(str(train_dir/'*/*'), shuffle=False)\n",
        "train_ds = train_ds.shuffle(len(train_ds), reshuffle_each_iteration=False)\n",
        "train_ds = train_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# train_ds = train_ds.cache('tmp')\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_ds_length = len(train_ds)\n",
        "\n",
        "val_ds = tf.data.Dataset.list_files(str(val_dir/'*/*'), shuffle=False)\n",
        "val_ds = val_ds.shuffle(len(val_ds), reshuffle_each_iteration=False)\n",
        "val_ds = val_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache()\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "val_ds_length = len(val_ds)\n",
        "\n",
        "print(f'train_ds_length: {train_ds_length}, val_ds_length: {val_ds_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI5l32JR-2_m",
        "outputId": "122621d8-e326-4ec9-8a05-53d3053ec4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16000 2000\n",
            "{'fake': 0, 'real': 1}\n",
            "train_ds_length: 1000, val_ds_length: 125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_ds.take(1):\n",
        "  pass\n",
        "# plt.figure(figsize=(32,10))\n",
        "plt.figure(figsize=(8,2))\n",
        "for i in range(4):\n",
        "  plt.subplot(1,4,i+1)\n",
        "  plt.imshow(x[i])\n",
        "  plt.axis('off')\n",
        "  plt.title(classes[tf.argmax(y[i])])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "0L_ET1OcXASq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn_relu(filters,x,idx,label):\n",
        "    x = SeparableConv2D(filters, 3, padding='same', kernel_initializer='he_uniform',name=f'conv_{idx}{label}')(x)\n",
        "    x = BatchNormalization(name=f'bn_{idx}{label}')(x)\n",
        "    x = ReLU(name=f'relu_{idx}{label}')(x)\n",
        "    # x = Dropout(rate=0.1,name=f'dropout_{idx}{label}')(x)\n",
        "    return x\n",
        "\n",
        "def create_model(input_shape = (256, 256, 3)):\n",
        "  input_layer = Input(input_shape,name='input')\n",
        "  n_filters = 8\n",
        "  x = input_layer\n",
        "  for i in range(5):\n",
        "    x = conv_bn_relu(n_filters,x,i,'a')\n",
        "    x = conv_bn_relu(n_filters,x,i,'b')\n",
        "    x = MaxPooling2D(name=f'maxpool_{i}')(x)\n",
        "    n_filters = int(n_filters*2)\n",
        "  # x = Flatten(name='flatten')(x)\n",
        "  x = GlobalAveragePooling2D(name='global_pool')(x)\n",
        "  # x = Dense(32,activation='relu',name='dense_0')(x)\n",
        "  # x = Dropout(rate=0.2,name=f'dropout_dense')(x)\n",
        "  x = Dense(2,activation='softmax',name='dense_0', dtype = 'float32')(x)\n",
        "\n",
        "  model = Model(inputs = [input_layer], outputs = [x])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "gpjsPNGnN5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "epochs = 50\n",
        "rLR_patience = 5\n",
        "es_patience = 10\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "n_filters = 8\n",
        "model_type = noise_type\n",
        "\n",
        "model_filename = f'{model_type}_sg3=20k_n={n_filters}_epoch={epochs}_lr={lr:.0e}'\n",
        "\n",
        "model_path = f'saved/models/{model_filename}.h5'\n",
        "history_path = f'saved/histories/{model_filename}.pkl'\n",
        "pathlib.Path('saved/models').mkdir(exist_ok=True,parents=True)\n",
        "pathlib.Path('saved/histories').mkdir(exist_ok=True,parents=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose = 1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', mode = 'max', factor=1/np.sqrt(10), patience = rLR_patience, min_lr=1e-6, verbose = 1)\n",
        "earlystopper = EarlyStopping(monitor='val_accuracy', mode = 'max', patience = es_patience, verbose=1)\n",
        "callbacks = [checkpoint, reduce_lr, earlystopper]\n",
        "\n",
        "model = create_model()\n",
        "print(model.count_params())\n",
        "optimizer = RMSprop(learning_rate = lr)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "hist = model.fit(train_ds, epochs = epochs, validation_data = val_ds, callbacks = callbacks, verbose = 1)\n",
        "with open(history_path, 'wb') as file_pi:\n",
        "  pickle.dump(hist.history, file_pi)"
      ],
      "metadata": {
        "id": "kB6W8l-ISOdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_path)\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BwbjCsTyWnRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iGNTsedWwI2",
        "outputId": "f74aaee1-58be-449c-87ed-df434961c3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = tf.data.Dataset.list_files(str(test_dir/'*/*'), shuffle=False)\n",
        "test_ds = test_ds.shuffle(len(test_ds), reshuffle_each_iteration=False)\n",
        "test_ds = test_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache()\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "test_ds_length = len(test_ds)\n",
        "\n",
        "print(f'test_ds_length: {test_ds_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5nYFA3rAcCr",
        "outputId": "e0ecb3f2-2f1b-44d9-afc3-c7ee8ff43fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_ds_length: 63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFN7vNbfASEP",
        "outputId": "c0401ab4-a310-4d18-8f81-c419c76bf62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 4s 57ms/step - loss: 0.7933 - accuracy: 0.5970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7933207750320435, 0.597000002861023]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKnzfv9M6yUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}