{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMin46bR2iXw7NAE7jv3/6N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SarveshPatil99/Adversarial-Robustness-Enhancement/blob/main/TML_train_mix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1-8d_JyH50-b23vnQXSn2BLM1E4Mh-0k5/view?usp=drive_link\n",
        "!unzip -q speckle_0.01.zip"
      ],
      "metadata": {
        "id": "Xu3MW73mlvLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/uc?id=18MZZwgKjTGlB2Y4Esw_lRIvbpOaANbnh\n",
        "!unzip -q original_dataset_stylegan3_10000.zip"
      ],
      "metadata": {
        "id": "UDVaukQolwBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q list_files.zip"
      ],
      "metadata": {
        "id": "jKZqH93FQblP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import os\n",
        "from tensorflow.keras.layers import Input, Conv2D, SeparableConv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, ReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "from random import sample, shuffle, seed\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')"
      ],
      "metadata": {
        "id": "--q0Na8a9wLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pathlib import Path\n",
        "# Path('list_files/train').mkdir(parents=True)\n",
        "# Path('list_files/val').mkdir(parents=True)\n",
        "# Path('list_files/test').mkdir(parents=True)\n",
        "\n",
        "# for ratio in np.arange(0,1.1,1/8):\n",
        "#   # print(ratio)\n",
        "#   for split in ['train', 'val', 'test']:\n",
        "#     total_files = []\n",
        "#     for label in ['real', 'fake']:\n",
        "#       og_files =  [f'original/{split}/{label}/{f}' for f in os.listdir(f'original/{split}/{label}')]\n",
        "#       noisy_files =  [f'noisy_data/speckle_0.01/{split}/{label}/{f}' for f in os.listdir(f'noisy_data/speckle_0.01/{split}/{label}')]\n",
        "#       seed(0)\n",
        "#       total_files += sample(og_files, int(ratio * len(og_files)))\n",
        "#       seed(0)\n",
        "#       total_files += sample(noisy_files, int((1 - ratio) * len(noisy_files)))\n",
        "#     seed(0)\n",
        "#     shuffle(total_files)\n",
        "#     print(f'{split}_og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}')\n",
        "#     with open(f'list_files/{split}/og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}.txt', 'w') as f:\n",
        "#       f.write('\\n'.join(total_files))\n",
        "\n",
        "# Assert files\n",
        "for ratio in np.arange(0,1.1,1/8):\n",
        "  print(ratio)\n",
        "  for split in ['train', 'val', 'test']:\n",
        "    print(split, end = ' ')\n",
        "    with open(f'list_files/{split}/og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}.txt', 'r') as f:\n",
        "      lines = f.read().split('\\n')\n",
        "      len_real = len([f for f in lines if 'real' in f])\n",
        "      len_fake = len([f for f in lines if 'fake' in f])\n",
        "      len_og = len([f for f in lines if 'original' in f])\n",
        "      len_noisy = len([f for f in lines if 'noisy_data' in f])\n",
        "      # print(f\"{len_real} real {len_fake} fake\")\n",
        "      print(f\"{len_og} og {len_noisy} noisy\")\n",
        "      assert len_real == len_fake\n",
        "      assert len_og / (len_og + len_noisy) == ratio"
      ],
      "metadata": {
        "id": "tlT88KscDmmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise_type = 'speckle_0.01'\n",
        "classes = np.array(['real', 'fake'])\n",
        "img_height = 256\n",
        "img_width = 256\n",
        "batch_size = 16\n",
        "ratio = 0 # 0, 0.125, 0.25, 0.5, 0.625, 0.75, 0.875, 1\n",
        "\n",
        "def create_label(image_path):\n",
        "  class_name = tf.strings.split(image_path,'/')[-2]\n",
        "  return tf.cast(classes == class_name,tf.float32)\n",
        "\n",
        "def load(image_path):\n",
        "\n",
        "  image = tf.io.read_file(image_path)\n",
        "  image = tf.image.decode_png(image) / 255\n",
        "  # image = tf.image.resize(image, [img_height, img_width])\n",
        "\n",
        "  label = create_label(image_path)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "with open(f'list_files/train/og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}.txt', 'r') as f:\n",
        "  train_files = f.read().split('\\n')\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
        "train_ds = train_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.batch(batch_size)\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_ds_length = len(train_ds)\n",
        "\n",
        "with open(f'list_files/val/og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}.txt', 'r') as f:\n",
        "  val_files = f.read().split('\\n')\n",
        "val_ds = tf.data.Dataset.from_tensor_slices(val_files)\n",
        "val_ds = val_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(batch_size)\n",
        "val_ds = val_ds.cache()\n",
        "val_ds_length = len(val_ds)\n",
        "\n",
        "with open(f'list_files/test/og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}.txt', 'r') as f:\n",
        "  test_files = f.read().split('\\n')\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(test_files)\n",
        "test_ds = test_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "test_ds = test_ds.cache()\n",
        "test_ds_length = len(test_ds)\n",
        "\n",
        "print(f'train_ds_length: {train_ds_length}, val_ds_length: {val_ds_length}, test_ds_length: {test_ds_length}')"
      ],
      "metadata": {
        "id": "MLG9nKGvN-4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_ds.take(1):\n",
        "  pass\n",
        "plt.figure(figsize=(8,2))\n",
        "for i in range(4):\n",
        "  plt.subplot(1,4,i+1)\n",
        "  plt.imshow(x[i])\n",
        "  plt.axis('off')\n",
        "  plt.title(classes[tf.argmax(y[i])])\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "0L_ET1OcXASq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn_relu(filters,x,idx,label):\n",
        "  x = SeparableConv2D(filters, 3, padding='same', kernel_initializer='he_uniform',name=f'conv_{idx}{label}')(x)\n",
        "  x = BatchNormalization(name=f'bn_{idx}{label}')(x)\n",
        "  x = ReLU(name=f'relu_{idx}{label}')(x)\n",
        "  # x = Dropout(rate=0.1,name=f'dropout_{idx}{label}')(x)\n",
        "  return x\n",
        "\n",
        "def create_model(input_shape = (256, 256, 3)):\n",
        "  input_layer = Input(input_shape,name='input')\n",
        "  n_filters = 16\n",
        "  x = input_layer\n",
        "  for i in range(5):\n",
        "    x = conv_bn_relu(n_filters,x,i,'a')\n",
        "    x = conv_bn_relu(n_filters,x,i,'b')\n",
        "    x = MaxPooling2D(name=f'maxpool_{i}')(x)\n",
        "    n_filters = int(n_filters*2)\n",
        "  # x = Flatten(name='flatten')(x)\n",
        "  x = GlobalAveragePooling2D(name='global_pool')(x)\n",
        "  # x = Dense(32,activation='relu',name='dense_0')(x)\n",
        "  # x = Dropout(rate=0.2,name=f'dropout_dense')(x)\n",
        "  x = Dense(2,activation='softmax',name='dense_0', dtype = 'float32')(x)\n",
        "\n",
        "  model = Model(inputs = [input_layer], outputs = [x])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "gpjsPNGnN5da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "epochs = 50\n",
        "rLR_patience = 5\n",
        "es_patience = 10\n",
        "loss = 'categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "n_filters = 16\n",
        "model_type = f'{noise_type}_og_{ratio*100:.1f}_noisy_{(1-ratio)*100:.1f}'\n",
        "\n",
        "model_filename = f'{model_type}_sg3=20k_n={n_filters}_epoch={epochs}_lr={lr:.0e}'\n",
        "\n",
        "model_path = f'saved/models/{model_filename}.h5'\n",
        "history_path = f'saved/histories/{model_filename}.pkl'\n",
        "pathlib.Path('saved/models').mkdir(exist_ok=True,parents=True)\n",
        "pathlib.Path('saved/histories').mkdir(exist_ok=True,parents=True)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_accuracy', mode='max', save_best_only=True, verbose = 1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', mode = 'max', factor=1/np.sqrt(10), patience = rLR_patience, min_lr=1e-6, verbose = 1)\n",
        "earlystopper = EarlyStopping(monitor='val_accuracy', mode = 'max', patience = es_patience, verbose=1)\n",
        "callbacks = [checkpoint, reduce_lr, earlystopper]\n",
        "\n",
        "model = create_model()\n",
        "print(model.count_params())\n",
        "optimizer = RMSprop(learning_rate = lr)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = metrics)\n",
        "hist = model.fit(train_ds, epochs = epochs, validation_data = val_ds, callbacks = callbacks, verbose = 1)\n",
        "with open(history_path, 'wb') as file_pi:\n",
        "  pickle.dump(hist.history, file_pi)"
      ],
      "metadata": {
        "id": "kB6W8l-ISOdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(model_path)\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BwbjCsTyWnRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model = load_model(model_path)\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "-iGNTsedWwI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_test_files = [f'original/test/real/{f}' for f in os.listdir(f'original/test/real')] + [f'original/test/fake/{f}' for f in os.listdir(f'original/test/fake')]\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(og_test_files)\n",
        "test_ds = test_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "test_ds = test_ds.cache()\n",
        "test_ds_length = len(test_ds)\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "OFN7vNbfASEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy https://drive.google.com/file/d/1l3Hjb4LemQNEj2Muf7T87dCAYHroafux/view?usp=sharing\n",
        "!unzip -q adversarial_fgsm_0.001_big_56.zip"
      ],
      "metadata": {
        "id": "OKnzfv9M6yUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_test_files = [f'adversarial_fgsm/test/real/{f}' for f in os.listdir(f'adversarial_fgsm/test/real')] + [f'adversarial_fgsm/test/fake/{f}' for f in os.listdir(f'adversarial_fgsm/test/fake')]\n",
        "test_ds = tf.data.Dataset.from_tensor_slices(og_test_files)\n",
        "test_ds = test_ds.map(load, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.batch(batch_size)\n",
        "test_ds = test_ds.cache()\n",
        "test_ds_length = len(test_ds)\n",
        "model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "muihEMC3U54v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f4Y7fpI_VEpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}